{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "k82Clww3VwkJ",
        "outputId": "eda9e668-1e38-4ae3-e165-fb7415db76d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/infy_merged_2.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b2ca8a10e570>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/infy_merged_2.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Rename columns properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/infy_merged_2.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/infy_merged_2.csv\", header=1)\n",
        "\n",
        "# Rename columns properly\n",
        "df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "# Drop any remaining NaN rows if needed\n",
        "df = df.dropna()\n",
        "\n",
        "# Check the first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVBbLvO7a3n4",
        "outputId": "62269346-c7e6-4160-a3f0-1b48de0dab1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "3z8Linu-a8sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "df[['Close', 'High', 'Low', 'Open', 'Volume']] = scaler.fit_transform(\n",
        "    df[['Close', 'High', 'Low', 'Open', 'Volume']]\n",
        ")"
      ],
      "metadata": {
        "id": "GRACqEKueCXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class StockTradingEnvNoSentiment(gym.Env):\n",
        "    def __init__(self, df):\n",
        "        super(StockTradingEnvNoSentiment, self).__init__()\n",
        "        self.df = df.reset_index()\n",
        "        self.current_step = 0\n",
        "        self.cash = 10000  # Initial balance\n",
        "        self.stock_held = 0\n",
        "        self.total_value = self.cash\n",
        "        self.max_steps = len(self.df) - 1\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.cash = 10000\n",
        "        self.stock_held = 0\n",
        "        self.total_value = self.cash\n",
        "        return self._next_observation()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        obs = self.df.iloc[self.current_step][['Close', 'Volume']].values\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        prev_total = self.total_value\n",
        "        price = self.df.iloc[self.current_step]['Close']\n",
        "\n",
        "        if action == 1 and self.cash >= price:  # Buy\n",
        "            self.stock_held += 1\n",
        "            self.cash -= price\n",
        "        elif action == 2 and self.stock_held > 0:  # Sell\n",
        "            self.stock_held -= 1\n",
        "            self.cash += price\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        self.total_value = self.cash + (self.stock_held * price)\n",
        "        profit = self.total_value - prev_total\n",
        "        reward = profit  # No sentiment influence\n",
        "\n",
        "        # Track trade history\n",
        "        if not hasattr(self, \"trade_history\"):\n",
        "            self.trade_history = []\n",
        "\n",
        "        self.trade_history.append({\n",
        "            \"Step\": self.current_step,\n",
        "            \"Date\": self.df.iloc[self.current_step]['Date'],\n",
        "            \"Action\": \"Buy\" if action == 1 else \"Sell\" if action == 2 else \"Hold\",\n",
        "            \"Stock Held\": self.stock_held,\n",
        "            \"Cash\": self.cash,\n",
        "            \"Total Value\": self.total_value,\n",
        "            \"Stock Price\": price\n",
        "        })\n",
        "\n",
        "        return self._next_observation(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Step: {self.current_step}, Cash: {self.cash}, Stocks: {self.stock_held}, Total Value: {self.total_value}\")\n"
      ],
      "metadata": {
        "id": "mAdZvTQKdxCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "neyPWmy5eq9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = StockTradingEnvNoSentiment(df)\n",
        "dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "num_episodes = 100\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.1\n",
        "memory = deque(maxlen=1000)"
      ],
      "metadata": {
        "id": "KL18uZwoet9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32)\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(dqn(state)).item()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(memory) > 32:\n",
        "            batch = random.sample(memory, 32)\n",
        "\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "            states = torch.stack(states)\n",
        "            actions = torch.tensor(actions, dtype=torch.int64)\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "            next_states = torch.stack(next_states)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "            q_values = dqn(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            next_q_values = dqn(next_states).max(1)[0].detach()\n",
        "            target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "            loss = loss_fn(q_values, target_q_values)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUOEbNOze0RD",
        "outputId": "29ba0e94-3e67-4972-cffc-480626c2f6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 20.038962444188655\n",
            "Episode 2, Total Reward: 3.967315986399626\n",
            "Episode 3, Total Reward: 0.4021821436163009\n",
            "Episode 4, Total Reward: 25.015680785119912\n",
            "Episode 5, Total Reward: 12.926405707295999\n",
            "Episode 6, Total Reward: 11.288920732422412\n",
            "Episode 7, Total Reward: 14.845163845493516\n",
            "Episode 8, Total Reward: 18.56522567859065\n",
            "Episode 9, Total Reward: -15.349400145725667\n",
            "Episode 10, Total Reward: -18.167089078231584\n",
            "Episode 11, Total Reward: 18.929624808319204\n",
            "Episode 12, Total Reward: 23.89604348750072\n",
            "Episode 13, Total Reward: 2.6697660980535147\n",
            "Episode 14, Total Reward: 10.410655273974044\n",
            "Episode 15, Total Reward: 5.125544294005522\n",
            "Episode 16, Total Reward: 13.950859675340325\n",
            "Episode 17, Total Reward: 17.658680268919852\n",
            "Episode 18, Total Reward: 2.867424174100961\n",
            "Episode 19, Total Reward: -3.8365747530533554\n",
            "Episode 20, Total Reward: -19.60394644349617\n",
            "Episode 21, Total Reward: 4.809046560876595\n",
            "Episode 22, Total Reward: -6.713278512961551\n",
            "Episode 23, Total Reward: 15.013184018238462\n",
            "Episode 24, Total Reward: -4.530828561963062\n",
            "Episode 25, Total Reward: -0.8019541925496014\n",
            "Episode 26, Total Reward: 9.317957469684188\n",
            "Episode 27, Total Reward: -20.306863331094064\n",
            "Episode 28, Total Reward: -22.477628849010216\n",
            "Episode 29, Total Reward: 22.688724530582476\n",
            "Episode 30, Total Reward: -35.01659950450812\n",
            "Episode 31, Total Reward: -9.70368239225536\n",
            "Episode 32, Total Reward: -15.955974029842764\n",
            "Episode 33, Total Reward: -30.729478327142715\n",
            "Episode 34, Total Reward: -28.50505099653674\n",
            "Episode 35, Total Reward: 18.431037345593722\n",
            "Episode 36, Total Reward: -13.99336717343067\n",
            "Episode 37, Total Reward: -14.005074142127341\n",
            "Episode 38, Total Reward: 54.377412478466795\n",
            "Episode 39, Total Reward: -26.912263229958626\n",
            "Episode 40, Total Reward: 10.252878469977077\n",
            "Episode 41, Total Reward: 4.221307995901952\n",
            "Episode 42, Total Reward: -1.1117917636347556\n",
            "Episode 43, Total Reward: 8.244300012309395\n",
            "Episode 44, Total Reward: 14.74061270625134\n",
            "Episode 45, Total Reward: -13.854823817253418\n",
            "Episode 46, Total Reward: 25.191972911621633\n",
            "Episode 47, Total Reward: -43.61164854948947\n",
            "Episode 48, Total Reward: 3.916266221383921\n",
            "Episode 49, Total Reward: -8.770741339401866\n",
            "Episode 50, Total Reward: -6.329242103836805\n",
            "Episode 51, Total Reward: -40.6774524437169\n",
            "Episode 52, Total Reward: 5.2725765325358225\n",
            "Episode 53, Total Reward: 2.4347735472965724\n",
            "Episode 54, Total Reward: 60.15541819945065\n",
            "Episode 55, Total Reward: -18.34246532688121\n",
            "Episode 56, Total Reward: -54.5535501891336\n",
            "Episode 57, Total Reward: 3.4254216363679006\n",
            "Episode 58, Total Reward: -0.02592989017830405\n",
            "Episode 59, Total Reward: -12.865558804447573\n",
            "Episode 60, Total Reward: -6.824891705497066\n",
            "Episode 61, Total Reward: -7.264548635697793\n",
            "Episode 62, Total Reward: -47.307130595334456\n",
            "Episode 63, Total Reward: -21.500125429076434\n",
            "Episode 64, Total Reward: -10.208163114966737\n",
            "Episode 65, Total Reward: -14.306065529000989\n",
            "Episode 66, Total Reward: 16.419160488141642\n",
            "Episode 67, Total Reward: 61.82094502799191\n",
            "Episode 68, Total Reward: -78.3070976964791\n",
            "Episode 69, Total Reward: 26.557678180261064\n",
            "Episode 70, Total Reward: 9.431332923317314\n",
            "Episode 71, Total Reward: 16.62858624103137\n",
            "Episode 72, Total Reward: 8.544678378628305\n",
            "Episode 73, Total Reward: 65.74245901917311\n",
            "Episode 74, Total Reward: 62.770434464790014\n",
            "Episode 75, Total Reward: -70.09549885943125\n",
            "Episode 76, Total Reward: -82.8355474946402\n",
            "Episode 77, Total Reward: 15.261527628146723\n",
            "Episode 78, Total Reward: -12.819406795817486\n",
            "Episode 79, Total Reward: -27.88252569602264\n",
            "Episode 80, Total Reward: 11.515447383168066\n",
            "Episode 81, Total Reward: -29.25612955912402\n",
            "Episode 82, Total Reward: 44.95269077404009\n",
            "Episode 83, Total Reward: 6.266781442622232\n",
            "Episode 84, Total Reward: 3.9825978483913786\n",
            "Episode 85, Total Reward: -58.98913701294077\n",
            "Episode 86, Total Reward: 44.0925157163565\n",
            "Episode 87, Total Reward: -46.62473026933367\n",
            "Episode 88, Total Reward: 20.127090645408316\n",
            "Episode 89, Total Reward: -6.499394248367025\n",
            "Episode 90, Total Reward: 4.728032934646762\n",
            "Episode 91, Total Reward: 47.15670832979595\n",
            "Episode 92, Total Reward: 67.57348666436519\n",
            "Episode 93, Total Reward: -24.117278309080575\n",
            "Episode 94, Total Reward: -97.95492440803719\n",
            "Episode 95, Total Reward: -0.47829507001370075\n",
            "Episode 96, Total Reward: 13.819181018463496\n",
            "Episode 97, Total Reward: -5.377426810344332\n",
            "Episode 98, Total Reward: -12.53706096919268\n",
            "Episode 99, Total Reward: -58.3809924312136\n",
            "Episode 100, Total Reward: -7.326135814008012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert trade history to DataFrame\n",
        "trade_log = pd.DataFrame(env.trade_history)\n",
        "\n",
        "# Save to CSV\n",
        "trade_log.to_csv(\"trade_history2.csv\", index=False)\n",
        "\n",
        "print(\"Trade history saved to trade_history.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuGnCoZwfyzZ",
        "outputId": "b37e1b0e-5d52-4f19-d870-030f812163f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trade history saved to trade_history.csv\n"
          ]
        }
      ]
    }
  ]
}